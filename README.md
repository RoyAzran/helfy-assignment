# Helfy Assignment - Full-Stack Authentication with TiDB CDC

A complete full-stack application demonstrating authentication, database integration, Change Data Capture (CDC), and real-time event processing with Apache Kafka.

## ðŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚â”€â”€â”€â”€â–¶â”‚     API     â”‚â”€â”€â”€â”€â–¶â”‚      TiDB Cluster           â”‚
â”‚  (Nginx)    â”‚     â”‚  (Node.js)  â”‚     â”‚  (PD + TiKV + TiDB)         â”‚
â”‚   :80       â”‚     â”‚   :3000     â”‚     â”‚       :4000                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                           â”‚
                           â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚                    â”‚     TiCDC       â”‚
                           â”‚                    â”‚  (CDC Engine)   â”‚
                           â”‚                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Apache Kafka  â”‚
           â”‚   (Broker)     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                     â”‚
         â–¼                     â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Consumer  â”‚      â”‚   Analytics  â”‚
    â”‚  (Node.js)  â”‚      â”‚   Systems    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸ“Š Services

| Service | Image | Purpose | Ports |
|---------|-------|---------|-------|
| **client** | nginx:alpine | Serves frontend HTML/JS | 80 |
| **api** | node:18-alpine | Backend REST API with authentication | 3000 |
| **pd** | pingcap/pd | TiDB Placement Driver - cluster metadata & scheduling | 2379 |
| **tikv** | pingcap/tikv | TiDB Key-Value storage engine - distributed data storage | - |
| **tidb** | pingcap/tidb | TiDB SQL layer - executes queries | 4000 |
| **ticdc** | pingcap/ticdc | Change Data Capture - captures ALL database changes in real-time | 8300 |
| **zookeeper** | wurstmeister/zookeeper | Kafka coordination & leader election | 2181 |
| **kafka** | wurstmeister/kafka | Distributed message broker - event streaming | 9092 |
| **consumer** | node:18-alpine | Kafka consumer - processes streamed events | - |
| **db-init** | mysql:5.7 | One-time DB schema initialization | - |
| **cdc-init** | pingcap/ticdc | One-time CDC changefeed setup | - |

## ðŸ”„ Data Flow & Event Pipeline

### Step-by-Step Process

```
1. USER LOGS IN
   â””â”€â–º Client sends credentials to API

2. API VALIDATES & UPDATES DATABASE
   â””â”€â–º INSERT/UPDATE in TiDB
   â””â”€â–º Token stored in database

3. TICDC CAPTURES CHANGE (Real-Time)
   â””â”€â–º Detects: "UPDATE users SET token = '...' WHERE id = 1"
   â””â”€â–º Converts to structured CDC event

4. KAFKA RECEIVES EVENTS
   â””â”€â–º Topic 1: "user-events" (from API)
   â””â”€â–º Topic 2: "tidb-cdc-events" (from TiCDC)

5. CONSUMER PROCESSES & LOGS
   â””â”€â–º Subscribes to both topics
   â””â”€â–º Logs events in structured JSON format (log4js)
   â””â”€â–º Enables monitoring, auditing, analytics
```

## â“ Why This Complex Pipeline Instead of Just Reading from Database?

### The Traditional Approach (âŒ Limited)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API    â”‚â”€â”€â”€â”€â”€â–¶â”‚  TiDB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problems:
- âŒ No audit trail of who changed what and when
- âŒ Can't track intermediate states
- âŒ No real-time event notifications
- âŒ API must constantly poll database
- âŒ Can't replay changes
- âŒ Difficult to integrate multiple systems
- âŒ Poor scalability for large data volumes
```

### The Event-Driven Approach (âœ… Enterprise)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   API    â”‚â”€â”€â”€â”€â”€â–¶â”‚  TiDB    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚  TiCDC   â”‚
                  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”Œâ”€â”€â–¶â”‚  Kafka   â”‚â—€â”€â”€â”
              â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
              â”‚                  â”‚
         â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚Consumer â”‚      â”‚ Analytics â”‚
         â”‚ Logging â”‚      â”‚  Engine   â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Benefits:
âœ… Complete audit trail of ALL changes
âœ… Timestamps for every operation
âœ… Can replay events from any point
âœ… Decoupled systems (Consumer doesn't block DB)
âœ… Real-time event streaming
âœ… Multiple consumers can process independently
âœ… Scales horizontally with Kafka partitions
âœ… Perfect for microservices architecture
```

## ðŸ’Ž Real-World Values This Pipeline Provides

### 1ï¸âƒ£ **Audit & Compliance**
```json
{
  "timestamp": "2024-12-05T10:30:00Z",
  "operation": "UPDATE",
  "table": "users",
  "user_id": 1,
  "old_values": {"token": null},
  "new_values": {"token": "abc-123"},
  "source": "user_login"
}
```
âœ… **Proof of what changed, when, and why**
âœ… **GDPR/PCI-DSS compliance**

### 2ï¸âƒ£ **Real-Time Notifications**
```
User logs in â†’ Token updated â†’ CDC event â†’ Kafka â†’ 
Consumer sends email/SMS/notification in REAL-TIME
(No polling needed!)
```

### 3ï¸âƒ£ **Multi-System Integration**
```
User updates profile in SYSTEM A
  â†“ (CDC event)
  â†“ (Kafka)
SYSTEM B auto-syncs customer data
SYSTEM C updates analytics
SYSTEM D triggers recommendation engine
(All happen in real-time without APIs talking to each other)
```

### 4ï¸âƒ£ **Event Replay & Recovery**
```
If a system crashes, it can:
1. Reconnect to Kafka
2. Replay events from last checkpoint
3. Recover to exact state without re-querying database
(No data loss, faster recovery)
```

### 5ï¸âƒ£ **Scalability**
```
Traditional API approach:
- 1000 users login â†’ 1000 direct database hits
- Database becomes bottleneck

Event-driven approach:
- 1000 users login â†’ Kafka handles all events
- Multiple consumers process independently
- Database load remains constant
- Each system processes at its own speed
```

### 6ï¸âƒ£ **Analytics & Business Intelligence**
```
Every database change is automatically captured
â†“
Kafka streams to data warehouse
â†“
Real-time dashboards showing:
- User login patterns
- Peak usage times
- Error rates
- System health metrics
(Without ANY queries hitting production database)
```

### 7ï¸âƒ£ **Decoupling & Microservices**
```
Without CDC:
API â†’ Auth Service â†’ User Service â†’ Analytics Service
(Tightly coupled, synchronous, slow)

With CDC:
API â†’ TiDB
        â†“ (CDC)
        â†“ (Kafka)
   Multiple independent consumers:
   - Auth Service
   - User Service
   - Analytics Service
   - Notification Service
   - Backup Service
(Loosely coupled, asynchronous, fast)
```

## ðŸ“ˆ Concrete Example: Login Event

### What Happens (Step by Step)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER CLICKS LOGIN (10:30:00.123)                             â”‚
â”‚    Input: admin / admin                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. API VALIDATES & UPDATES (10:30:00.456)                       â”‚
â”‚    - SELECT * FROM users WHERE username='admin'                 â”‚
â”‚    - Credentials valid âœ“                                         â”‚
â”‚    - Generate token: "abc-123-def-456"                          â”‚
â”‚    - UPDATE users SET token='abc-123-def-456' WHERE id=1        â”‚
â”‚    - Log to API logs: {action: LOGIN_SUCCESS, userId: 1}        â”‚
â”‚    - Send event to Kafka "user-events" topic                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. TIDB STORES UPDATE (10:30:00.789)                            â”‚
â”‚    - Token value committed to disk                               â”‚
â”‚    - Row version incremented                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. TICDC DETECTS CHANGE (10:30:00.890)                          â”‚
â”‚    - Row: users, ID: 1                                           â”‚
â”‚    - Operation: UPDATE                                           â”‚
â”‚    - Old: {id: 1, username: 'admin', token: null}              â”‚
â”‚    - New: {id: 1, username: 'admin', token: 'abc-123-def-456'} â”‚
â”‚    - Sends to Kafka "tidb-cdc-events" topic                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CONSUMER PROCESSES EVENTS (10:30:00.945)                     â”‚
â”‚    Receives 2 events:                                            â”‚
â”‚                                                                  â”‚
â”‚    A) User Event (from user-events topic):                      â”‚
â”‚       {timestamp, action: LOGIN_SUCCESS, userId: 1, ip: ...}    â”‚
â”‚                                                                  â”‚
â”‚    B) CDC Event (from tidb-cdc-events topic):                   â”‚
â”‚       {timestamp, operation: UPDATE, table: users,               â”‚
â”‚        oldData: {token: null}, newData: {token: abc-123...}}   â”‚
â”‚                                                                  â”‚
â”‚    Logs both in structured JSON format                          â”‚
â”‚    Can trigger: email, notifications, analytics, backup, etc.  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ðŸŽ¯ Key Differences

| Aspect | Simple DB Query | Event-Driven CDC |
|--------|-----------------|-----------------|
| **Real-time** | âŒ Polling required | âœ… Instant notification |
| **Audit Trail** | âŒ Manual logging | âœ… Automatic for ALL changes |
| **Scalability** | âŒ DB load increases | âœ… Horizontal scaling |
| **Multiple Consumers** | âŒ DB bottleneck | âœ… Independent processing |
| **Event Replay** | âŒ Not possible | âœ… Full history available |
| **Decoupling** | âŒ Tight coupling | âœ… Loose coupling |
| **Analytics** | âŒ Slow, impacts DB | âœ… Real-time, no impact |
| **Compliance** | âŒ Manual audit logs | âœ… Complete audit trail |

## ðŸš€ Quick Start

```bash
# Start everything with one command
docker-compose up --build

# Watch the pipeline in action
# Terminal 1: API logs
docker logs helfy-assignment-api-1 -f

# Terminal 2: Consumer logs
docker logs helfy-assignment-consumer-1 -f

# Terminal 3: Test login
curl -X POST http://localhost:3000/api/login \
  -H "Content-Type: application/json" \
  -d '{"username":"admin","password":"admin"}'
```

**See logs appear in BOTH terminals in real-time!**

## ðŸ“ Project Structure

```
helfy-assignment/
â”œâ”€â”€ client/                          # Frontend
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â”œâ”€â”€ index.html              # Login page
â”‚   â”‚   â”œâ”€â”€ dashboard.html          # Protected page
â”‚   â”‚   â”œâ”€â”€ app.js                  # Frontend logic
â”‚   â”‚   â””â”€â”€ styles.css              # Styling
â”‚   â”œâ”€â”€ nginx.conf                  # Nginx configuration
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ api/                             # Backend API
â”‚   â”œâ”€â”€ server.js                   # Express server + log4js logging
â”‚   â”œâ”€â”€ db.js                       # TiDB connection pool
â”‚   â”œâ”€â”€ logger.js                   # log4js setup
â”‚   â”œâ”€â”€ wait-for-services.js        # Service health checks
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ .env                        # Environment variables
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ consumer/                        # Kafka Consumer (Event Processor)
â”‚   â”œâ”€â”€ index.js                    # Kafka consumer + log4js logging
â”‚   â”œâ”€â”€ logger.js                   # log4js setup
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ db/
â”‚   â””â”€â”€ schema.sql                  # Database schema & default user
â”œâ”€â”€ docker-compose.yml              # Complete infrastructure
â””â”€â”€ README.md                        # This file
```

## ðŸ”Œ API Endpoints

### POST /api/login
Authenticate user and receive authentication token.

**Request:**
```bash
curl -X POST http://localhost:3000/api/login \
  -H "Content-Type: application/json" \
  -d '{
    "username": "admin",
    "password": "admin"
  }'
```

**Response:**
```json
{
  "token": "550e8400-e29b-41d4-a716-446655440000",
  "userId": 1,
  "username": "admin"
}
```

### GET /health
Check system health status.

**Response:**
```json
{
  "status": "healthy",
  "database": "connected",
  "kafka": true
}
```

## ðŸ“ Logging Format

All logs use **log4js** in JSON format for easy parsing and monitoring.

### API Login Event Log
```json
{
  "timestamp": "2024-12-05T10:30:00.123Z",
  "userId": 1,
  "action": "LOGIN_SUCCESS",
  "ipAddress": "::ffff:172.18.0.1"
}
```

### Consumer User Event Log
```json
{
  "timestamp": "2024-12-05T10:30:00.456Z",
  "source": "kafka-user-events",
  "event": "LOGIN_SUCCESS",
  "userId": 1,
  "username": "admin",
  "ip": "::ffff:172.18.0.1"
}
```

### Consumer CDC Event Log
```json
{
  "timestamp": "2024-12-05T10:30:00.890Z",
  "source": "TiDB-CDC",
  "topic": "tidb-cdc-events",
  "operation": "UPDATE",
  "database": "test",
  "table": "users",
  "data": {
    "id": 1,
    "username": "admin",
    "token": "550e8400-e29b-41d4-a716-446655440000"
  }
}
```

## ðŸ—„ï¸ Database Schema

```sql
CREATE TABLE users (
    id INT PRIMARY KEY AUTO_INCREMENT,
    username VARCHAR(255) UNIQUE NOT NULL,
    password VARCHAR(255) NOT NULL,
    token VARCHAR(255),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- Default user for testing
INSERT INTO users (username, password) VALUES ('admin', 'admin');
```

## ðŸ› ï¸ Technology Stack

| Layer | Technologies |
|-------|--------------|
| **Frontend** | HTML5, CSS3, Vanilla JavaScript, Nginx |
| **Backend** | Node.js 18, Express.js 4 |
| **Database** | TiDB (MySQL-compatible), TiKV, PD |
| **CDC/Streaming** | TiCDC, Apache Kafka, Zookeeper |
| **Logging** | log4js (structured JSON logging) |
| **Container** | Docker, Docker Compose |
| **Authentication** | UUID tokens, HTTP headers |

## ðŸ” Security Features

- âœ… Username/password authentication
- âœ… UUID token generation
- âœ… Token-based session management
- âœ… Token stored in database
- âœ… Complete audit trail of all changes
- âœ… IP address logging for security monitoring

## ðŸ“Š Environment Variables

```env
# API Configuration
PORT=3000

# TiDB Configuration
TIDB_HOST=tidb
TIDB_PORT=4000
TIDB_USER=root
TIDB_PASSWORD=
TIDB_DATABASE=test
TIDB_SSL=false

# Kafka Configuration
KAFKA_BROKERS=kafka:9092
```

## ðŸš¦ Monitoring & Debugging

### View API Logs
```bash
docker logs helfy-assignment-api-1 -f
```

### View Consumer Logs
```bash
docker logs helfy-assignment-consumer-1 -f
```

### Check TiDB Connection
```bash
docker exec -it helfy-assignment-tidb-1 mysql -uroot -h127.0.0.1 -Dtest -e "SELECT * FROM users;"
```

### Check Kafka Topics
```bash
docker exec -it helfy-assignment-kafka-1 \
  /opt/kafka/bin/kafka-topics.sh \
  --list --bootstrap-server localhost:9092
```

### Check CDC Status
```bash
docker exec -it helfy-assignment-api-1 node -e "
const http = require('http');
http.get('http://ticdc:8300/api/v1/changefeeds', res => {
  let data = '';
  res.on('data', chunk => data += chunk);
  res.on('end', () => console.log(JSON.parse(data)));
});
"
```

## ðŸ§ª Testing the Pipeline

### 1. Start Everything
```bash
docker-compose up --build -d
sleep 30  # Wait for services
```

### 2. Open Multiple Terminals
```bash
# Terminal 1: Watch API logs
docker logs helfy-assignment-api-1 -f

# Terminal 2: Watch Consumer logs  
docker logs helfy-assignment-consumer-1 -f

# Terminal 3: Test logins
```

### 3. Test Login Multiple Times
```bash
for i in {1..3}; do
  echo "Login attempt $i:"
  curl -X POST http://localhost:3000/api/login \
    -H "Content-Type: application/json" \
    -d '{"username":"admin","password":"admin"}'
  echo ""
  sleep 2
done
```

**Result:** See logs appear in BOTH Terminal 1 and Terminal 2 in real-time!

## ðŸ›‘ Stopping & Cleanup

```bash
# Stop all containers
docker-compose down

# Stop and remove volumes (complete cleanup)
docker-compose down -v

# View all containers status
docker-compose ps
```

## ðŸŽ“ Learning Points

This project demonstrates:

1. **Full-Stack Development** - Frontend, API, Database integration
2. **Microservices Architecture** - Decoupled, scalable components
3. **Event-Driven Systems** - Real-time event processing
4. **CDC Technology** - Database change capture & streaming
5. **Message Queues** - Kafka for async communication
6. **DevOps/SRE** - Docker, Compose, monitoring, logging
7. **Structured Logging** - Enterprise-grade log4js usage
8. **Audit & Compliance** - Complete change tracking
9. **Scalability Patterns** - Horizontal scaling with Kafka

## ðŸ“š Further Reading

- **TiDB**: https://docs.pingcap.com/tidb/stable
- **TiCDC**: https://docs.pingcap.com/tidb/stable/ticdc-overview
- **Apache Kafka**: https://kafka.apache.org/documentation
- **log4js**: https://log4js-node.github.io/log4js-node/
- **Event-Driven Architecture**: https://www.confluent.io/blog/event-driven-architecture/

## ðŸ“„ License

This is a learning project for educational purposes.
